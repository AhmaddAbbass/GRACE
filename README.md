Hereâ€™s a clean, **complete** `README.md` you can drop in the repo.
I kept your original narrative (with the same image refs) and added a thorough tour of the codebase + step-by-step run instructions.

---

# GRACE: Graph-Based Reasoning Across Computational Environments ğŸ§ 

## ğŸ‘¥ Authors

Ahmad Abbas â€¢ Nour Fakih â€¢ Tamara Fakih

## ğŸŒ Overview

**GRACE** (Graph Reasoning & Algorithm Composition Engine) is a **multi-domain reasoning assistant** that organizes procedural knowledge â€” such as *recipes*, *repair guides*, or *algorithms* â€” into **knowledge graphs**. It uses those graphs to **retrieve**, **compose**, and **explain** step-by-step solutions.

> Think of GRACE as a *knowledge graph for doing things* â€” not just answering questions, but showing how the answer is built.

## Motivation & Origin ğŸ’¡

We began with a simple hunch: **algorithms across different domains are connected**. Their ideas often influence, reuse, or compose one another. A strategy in one setting (e.g., divide-and-conquer) can inspire a plan in another (e.g., staging tasks in a kitchen). We wanted to capture these interdependencies explicitlyâ€”to see how one procedure's logic can enable or inform anotherâ€”and the most natural representation was a graph.

![GRACE Graph Structure](images/graph.png)

### Graph Structure ğŸ•¸ï¸

* **Nodes**: meaningful units (steps, tools, resources, variables, functions)
* **Edges**: relationships (depends-on, uses, produces, precondition-of, part-of, similar-to)
* **Algorithms** emerge as paths/subgraphs; cross-links reveal shared motifs and transferable reasoning.

## Why We Started with Everyday Domains ğŸ³

We scoped this during a 12-hour hackathon, so we reframed â€œalgorithmâ€ as **any everyday procedure** and started with **cooking**:

* A **recipe is an algorithm**: inputs (ingredients), constraints (diet/time), steps (method), output (dish).
* The domain offers clear relationships: shared ingredients, substitutions, techniques, cuisines.
* Edges like `contains`, `substitutes-for`, `similar-to`, `belongs-to`, `requires-tool` make the structure tangible.
* The result is explainable and intuitive for demo and UX.

## GRACE Capabilities ğŸš€

* **ğŸ” Retrieve** relevant neighborhoods for a userâ€™s query (e.g., â€œmushroom + chickenâ€)
* **ğŸ’¡ Compose** new procedures by recombining compatible nodes/edges
* **ğŸ“– Explain** proposals by showing exact nodes/edges (e.g., shiitake â†’ cremini via `substitutes-for`)

## Extending Beyond Cooking ğŸ”§

We apply the same approach to **repair/maintenance** (apparel fixes, appliances, phones, hardware). Each guide is a procedure:

* **Inputs**: tools/materials
* **Constraints**: safety, prerequisites, device model
* **Steps**: ordered instructions
* **Output**: repaired item

This yields parallel graphs where nodes/edges mirror cooking, enabling transfer of procedural motifs (safety checks â†” preconditions, tool usage â†” resource dependencies, step ordering â†” control flow).

## Roadmap ğŸ—ºï¸

**Short term**: build strong graph backbones for cooking/repair; surface explainable traversals in the UI
**Medium term**: scale to CS algorithms; link shared strategies (greedy/DP/recursion); compare trade-offs
**Long term**: cross-domain library of algorithmic motifs; reuse motifs to compose new solutions; transparent â€œwhy-pathâ€ justifications

## Future Architecture: End-to-End Problem Solver ğŸ¤–

We envision GRACE as an agentic orchestrator over retrieval + planning + explanation:

```
User Query
  â”‚
  â–¼
GRACE Orchestrator
  â”‚
  â”œâ”€â”€> Graph Retriever (multi-hop, subgraphs)
  â”œâ”€â”€> Context Composer (entities + community summaries)
  â””â”€â”€> LLM Planner (prompt-based reasoning)
  â”‚
  â–¼
Structured Answer (step-by-step + graph citations)
```

![GRACE Model Architecture](images/arch.png)

---

# Repository Tour

> KGs are *built artifacts*. You ingest text into HiRAG â†’ it writes caches under `kgs/<graph>/<mode>/.hi_cache/` and an `index.json` for the UI.

```
GRACE/
â”œâ”€ books/                       # Source texts for ingestion
â”‚  â”œâ”€ food.txt
â”‚  â””â”€ books_3a_janab/...
â”‚
â”œâ”€ rag/                         # RAG toolkit (thin wrapper over HiRAG)
â”‚  â”œâ”€ __init__.py               # RAG class: build/retrieve/answer/history/dump_index
â”‚  â”œâ”€ config.py|config.yaml     # Resolves paths, embeddings, HiRAG knobs
â”‚  â”œâ”€ embeddings.py             # E5 or OpenAI embedding helpers
â”‚  â”œâ”€ rag_examples.py           # Demo scripts (context prints + tiny chatbot loop)
â”‚  â”œâ”€ runners/
â”‚  â”‚  â”œâ”€ base.py                # Abstract runner interface
â”‚  â”‚  â””â”€ hirag.py               # HiRAGRunner: insert/query/answer + CSV parsing + dump_index
â”‚  â””â”€ vis/
â”‚     â””â”€ index_utils.py         # Builds UI graph payload from .hi_cache (GraphML + chunk store)
â”‚
â”œâ”€ server/                      # Flask backend (REST API over rag.RAG)
â”‚  â”œâ”€ app.py                    # Endpoints: /kgs, /data, /retrieve, /answer, /history, /.../stream
â”‚  â”œâ”€ agent.py                  # LLM summarizer + KG ranking (auto-route) cached in summary.txt
â”‚  â”œâ”€ utils.py                  # KG discovery, registry, history IO, RAG instance cache
â”‚  â”œâ”€ config.py|config.yaml     # Server settings (ports, CORS, summary model, top_k/m)
â”‚
â”œâ”€ kgx-ui/                      # Vite + React + TS graph explorer
â”‚  â”œâ”€ src/
â”‚  â”‚  â”œâ”€ api.ts                 # Client for backend; payload normalizers
â”‚  â”‚  â”œâ”€ App.tsx                # Shell: loads KGs, chat + graph, overlays
â”‚  â”‚  â”œâ”€ overlay.ts             # Build highlight sets (hits + reasoning path)
â”‚  â”‚  â”œâ”€ components/
â”‚  â”‚  â”‚  â”œâ”€ ChatDock.tsx        # Chat panel + send/replay; hooks graph highlights
â”‚  â”‚  â”‚  â”œâ”€ GraphCanvas.tsx     # ForceGraph2D layout; levels, colors, labels
â”‚  â”‚  â”‚  â”œâ”€ Legend.tsx          # Palette + path/bridge legend
â”‚  â”‚  â”‚  â””â”€ SideInfo.tsx        # Node/edge drawer (metadata + source chunks)
â”‚  â”‚  â”œâ”€ theme.css              # Layout + styling
â”‚  â”‚  â””â”€ types.ts               # Shared UI types
â”‚  â”œâ”€ index.html, vite.config.ts, package.json, ...
â”‚
â”œâ”€ tools/
â”‚  â””â”€ rebuild_kv.py             # (utility) regenerate KV stores if needed (optional)
â”‚
â”œâ”€ kgs/                         # (generated) graphs live here after build
â”‚  â””â”€ cooking_kg/
â”‚     â””â”€ hi/
â”‚        â”œâ”€ .hi_cache/          # HiRAG caches (chunks, entities, graphml, reports)
â”‚        â”œâ”€ index.json          # UI graph payload (built via dump_index)
â”‚        â””â”€ history/            # Saved contexts for queries
â”‚
â”œâ”€ requirements.txt
â””â”€ README.md (this file)
```

### Data flow (end-to-end)

1. **Ingest** `books/*.txt` â†’ `RAG.build_from_file(...)` (HiRAG inserts + clusters).
2. **Caches** written under `kgs/<graph>/<mode>/.hi_cache/` (chunks, entities, community reports, GraphML).
3. **Dump** `index.json` (via `rag.dump_index`) for the UI.
4. **Serve** via Flask (`server/app.py`) â†’ `/kgs`, `/data/<kg>`, `/retrieve`, `/answer`.
5. **UI** (`kgx-ui`) fetches `/kgs` + `index.json`, renders graph, calls `/retrieve`/`/answer`.
6. Each query writes `history/<run_id>/<qid>/context.json` for replay + highlighting.

---

# Setup & Run

> Backend (Flask + HiRAG) is pure Python; Frontend is a Vite/React workspace.
> No `node_modules` or `.venv` are checked in, so each fresh clone restores them.

## 0) Prereqs

* **Python 3.10+**
* **Node 18+** (for the UI)
* **OpenAI API key** (default embedding is OpenAI; you can switch to local E5 in `rag/config.yaml`)

## 1) Backend setup

From repo root:

```bash
python -m venv .venv
# macOS/Linux
source .venv/bin/activate
# Windows (PowerShell)
# .venv\Scripts\Activate.ps1

pip install -r requirements.txt
```

Create a **`.env`** at the repo root (used by `rag/config.py`):

```env
OPENAI_API_KEY=sk-...
```

> Want to avoid OpenAI? Edit `rag/config.yaml`:
>
> ```yaml
> default_embedding:
>   class: e5
>   model: intfloat/multilingual-e5-base
> ```
>
> and install `sentence-transformers`.

## 2) Build a Knowledge Graph

Build from the supplied book(s) and dump the UI index:

```bash
# Option A: ingest one file
python -m rag.cli --book books/food.txt

# Option B: ingest every *.txt under books/
python -m rag.cli
```

This creates:

```
kgs/cooking_kg/hi/.hi_cache/*   # chunks, entities, community reports, GraphML
kgs/cooking_kg/hi/index.json    # UI payload
```

> **FYI on utilities:** Thereâ€™s also a helper under `tools/rebuild_kv.py` you can run directly if you want to regenerate KV caches. The recommended canonical path remains `python -m rag.cli` (ingest + dump).

## 3) Launch the API

```bash
python -m server.app
# Binds to http://127.0.0.1:8000
```

**Smoke test:**

```bash
curl http://127.0.0.1:8000/kgs
curl -X POST http://127.0.0.1:8000/retrieve \
  -H "Content-Type: application/json" \
  -d '{"kg":"cooking_kg/hi","query":"Make jammy eggs"}'
```

## 4) Launch the UI

```bash
cd kgx-ui
npm install
npm run dev -- --host
# Visit the shown URL, e.g. http://127.0.0.1:5173
# The dropdown calls /kgs, so keep the backend running.
```

**Production build:**

```bash
npm run build
# (serve dist/ with your favorite static server or reverse-proxy to Flask)
```

---

# Backend: API Cheatsheet

* `GET /healthz` â†’ `{status:"ok"}`
* `GET /kgs?refresh=1` â†’ list discovered KGs (`kgs/<name>/<mode>/.hi_cache` present)
* `GET /kgs/<kg_id>/index.json` (alias `GET /data/<kg_id>`) â†’ graph payload for UI
* `GET /data/<kg_id>/<qid>/context.json?run_id=...` â†’ saved context record
* `POST /retrieve`
  Body: `{"kg":"cooking_kg/hi","query":"...", "top_k":8}`
  Returns context + node hits
* `POST /answer`
  Body: `{"kg":"cooking_kg/hi","query":"...","top_k":8,"model":"gpt-4o-mini"}`
  Returns `answer` **and** the same context payload
* **Auto-routing** (rank KGs via LLM summaries):

  * `POST /retrieve/auto` â†’ `{kg_rankings, results:[{kg_id, payload}]}`
  * `POST /answer/auto` â†’ idem + answers
* **Streaming** (SSE, one KG): `POST /answer/stream`
  Emits `context_progress`, an `answer` event (single chunk), and `done`.
* **History**:

  * `GET /history?run_id=...` â†’ paginated items for a run
  * `GET /history?kg=cooking_kg/hi` â†’ latest records across a KG
* **Summaries**:

  * `POST /summaries/refresh` `{kg_id, force?}` â†’ rebuilds `summary.txt`

---

# Configuration

## RAG config (`rag/config.yaml`)

* `logs_path` / `cache_dir` â†’ where KGs live. Default graph: `./kgs/cooking_kg`
* **Embeddings**: `default_embedding.class` = `openai` (default) or `e5`
* **HiRAG knobs**: chunk sizes, clustering algorithm (`leiden`), node2vec params, etc.

## Server config (`server/config.yaml`)

* `rag_config_path` â†’ points to the RAG config above
* `graphs_root` â†’ where to scan for KGs (`../rag/kgs` by default)
* `default_top_k` / `default_top_m` (for auto-routing fan-out)
* `summary.model` / `prompt` (used to summarize KGs for ranking)
* `app.cors_origins` â†’ CORS origins (default `["*"]`)

---

# How things are stitched together

* **Ingestion** (`rag/`):

  * `RAG.build_from_file()` chunks text, inserts into HiRAG, writes caches.
  * `RAG.dump_index()` calls `vis/index_utils.build_index_payload()` which loads `graph_chunk_entity_relation.graphml` + `kv_store_text_chunks.json`, computes centralities, tags retrieval hits, and writes a UI-friendly `index.json`.

* **Serving** (`server/`):

  * `utils.get_registry()` discovers KGs by looking for `.hi_cache` folders.
  * `utils.get_rag_for_kg(kg_id)` returns a cached `RAG` bound to that KGâ€™s directories.
  * `agent.summarize_kg()` builds short LLM summaries from sampled chunks and caches in `summary.txt`.
  * `agent.rank_kgs()` asks the LLM to return `[{kg_id, score}]`, with robust parsing and a fallback heuristic.

* **UI** (`kgx-ui/`):

  * `api.ts` normalizes payloads (works with `{nodes,edges}` or `{entity_graph:{...}}`).
  * `overlay.ts` maps retrieval â€œentitiesâ€ and â€œreasoning pathâ€ into node/edge highlights.
  * `GraphCanvas.tsx` uses ForceGraph2D, arranges nodes in level bands, and highlights hits/paths (bridges dashed).
  * `ChatDock.tsx` lets you send a message (`/answer` or `/retrieve`), then click bubbles to re-highlight the graph from saved history.

---

# Troubleshooting

* **Blank KG list in UI** â†’ make sure the backend is running on `:8000` and that you built a KG (`python -m rag.cli`).
* **OpenAI errors** â†’ ensure `.env` at repo root and `OPENAI_API_KEY` is set in the backend environment.
* **Local embeddings** â†’ switch to `e5` in `rag/config.yaml` and install `sentence-transformers`.
* **Large graphs slow** â†’ reduce `chunk_token_size`, increase `max_graph_cluster_size`, or trim books.
* **UI whitespace below graph** â†’ ensure the graph container stretches to full height; our `theme.css` already sets `.graph-viewport { height:100% }`. If you customize layout, keep `min-height:0` on flex children.

---

# License

MIT (or your preferred license â€“ fill this in)

---

# Acknowledgements

* **HiRAG** for hierarchical graph-augmented retrieval.
* Everyone who contributed examples, books, and test prompts during the hackathon.

---

**Happy graphing!** ğŸ•¸ï¸
